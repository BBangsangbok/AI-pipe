# Human vs. AI Story Classification Project ðŸ¤–âœï¸

**Course:** CAS2105 Mini AI Pipeline Project  
**Student:** Park Sang Uk (2024149038)

---

## 1. Task Definition

### Task Description

The goal of this project is to perform **binary text classification** to distinguish between stories written by humans and those generated by Artificial Intelligence (LLMs). The system receives a text passage as input and predicts whether it is **Human-written (Label 0)** or **AI-generated (Label 1)**.

### Motivation

As Large Language Models (LLMs) like GPT-4 and Claude become increasingly sophisticated, the line between human and machine creativity is blurring. Developing robust detection systems is critical for:

- **Academic Integrity:** Detecting AI plagiarism in essays and assignments.
- **Information Veracity:** Filtering mass-produced synthetic misinformation.
- **Content Moderation:** Verifying the authenticity of creative works.

### Input / Output

| Type | Description |
|------|-------------|
| **Input** | A raw text string (story or article snippet) |
| **Output** | A binary label (`0`: Human, `1`: AI) |

### Success Criteria

The project aims to build an AI pipeline (using a pre-trained Transformer) that significantly outperforms a simple rule-based baseline in terms of **Accuracy** on a held-out test set.

---

## 2. Dataset

### Source & Statistics

- **Source:** Hugging Face Datasets (`gsingh1-py/train`). This dataset typically contains prompts, human-written responses, and AI-generated variations.
- **Total Size:** 14,642 examples.
- **Class Balance:** The dataset was strictly balanced to a 1:1 ratio by down-sampling the AI-generated examples to match the number of human examples.

### Data Splits

The dataset was shuffled and split using a fixed random seed (`42`) for reproducibility:

| Split | Size | Purpose |
|-------|------|---------|
| Training Set (60%) | 8,784 examples | Used for model optimization |
| Validation Set (20%) | 2,929 examples | Used for monitoring training progress |
| Test Set (20%) | 2,929 examples | Held-out set for final evaluation |

### Preprocessing

- **Cleaning:** Handled `NaN` values by converting them to empty strings to prevent tokenizer errors.
- **Tokenization:** Used `DistilBertTokenizer` (WordPiece) with `do_lower_case=True`.
- **Truncation/Padding:** Fixed maximum sequence length of **256 tokens**. This length was chosen to optimize memory usage on a single GPU (RTX 3050 Laptop) while retaining sufficient context for classification.

---

## 3. Methods

### 3.1 NaÃ¯ve Baseline

To establish a minimum performance threshold, a simple heuristic method was implemented.

- **Method:** The baseline converts text to lowercase and scans for specific formatting artifacts often found in raw LLM outputs, such as Markdown headers (`##`) or bolding (`**`), and the keyword `"title"`.
- **Logic:** If any of these keywords are found, it predicts **AI (1)**; otherwise, it predicts **Human (0)**.
- **Why NaÃ¯ve?:** It relies purely on surface-level formatting cues and lacks any semantic understanding of the content.

### 3.2 AI Pipeline (DistilBERT)

An improved pipeline was built using the Hugging Face `transformers` library.

- **Model:** `distilbert-base-uncased` (A distilled version of BERT, 40% smaller and 60% faster)
- **Architecture:** `DistilBertForSequenceClassification` (Pre-trained Encoder + Linear Classification Head)

#### Training Configuration

| Parameter | Value |
|-----------|-------|
| Optimizer | AdamW |
| Learning Rate | 2e-5 |
| Batch Size | 16 |
| Epochs | 3 |
| Environment | NVIDIA RTX 3050 Laptop GPU |

---

## 4. Experiments and Results

### Quantitative Results

Both methods were evaluated on the same held-out **Test Set (2,929 examples)**. Since the dataset is perfectly balanced, **Accuracy** was used as the primary metric.

| Method | Accuracy |
|--------|----------|
| NaÃ¯ve Baseline | 92.49% |
| **AI Pipeline (DistilBERT)** | **100.00%** |

### Analysis

- The **NaÃ¯ve Baseline** achieved a surprisingly high accuracy of 92.49%. This indicates that the dataset relies heavily on formatting artifacts (e.g., AI formatting its output with Markdown).
- The **AI Pipeline** achieved perfect accuracy (100.00%), demonstrating that the model could capture all patterns, including those that the heuristic rules missed.

### Qualitative Analysis (Case Studies)

We analyzed specific cases where the Baseline failed but the AI Pipeline succeeded.

#### Case 1: Clean AI Text (False Negative in Baseline)

> **Text Snippet:** *"400000 A Tale of Three Housing Markets..."*

- **Ground Truth:** AI (1)
- **Baseline Prediction:** Human (0) âŒ
- **Analysis:** This text was generated by AI but lacked the specific Markdown keywords (`**`, `##`) required by the baseline. The AI model correctly identified the synthetic nature based on semantic patterns.

#### Case 2: Specific Artifacts (False Negative in Baseline)

> **Text Snippet:** *"Error: Error communicating with OpenAI: HTTPSConnectionPool..."*

- **Ground Truth:** AI (1)
- **Baseline Prediction:** Human (0) âŒ
- **Analysis:** The dataset contained an API error message labeled as AI. The baseline failed because "Error" is not in its keyword list. DistilBERT learned to associate this specific error pattern with the AI class.

#### Case 3: Formatted Human Text (False Positive in Baseline)

> **Text Snippet:** *"new video loaded... MAIN TITLE CARD Demon in the Freezer..."*

- **Ground Truth:** Human (0)
- **Baseline Prediction:** AI (1) âŒ
- **Analysis:** The text contained the phrase "MAIN TITLE CARD", which triggered the baseline's rule looking for the keyword "title". The AI pipeline correctly identified the context as a video transcript, distinguishing it from an AI-generated header.

---

## 5. Reflection and Limitations

### Successes

The transition from a rule-based approach to deep learning yielded perfect results. The AI pipeline successfully corrected all errors made by the baseline, proving the efficacy of **transfer learning**. Technically, optimizing the tokenizer (`batch_encode_plus`) and sequence length (256) allowed for efficient training on a consumer-grade GPU without memory errors.

### Critical Reflection & Limitations

While 100% accuracy is an ideal result, it paradoxically highlights significant limitations in the dataset:

1. **Dataset Simplicity:** The fact that a simple keyword baseline achieved ~92.5% suggests the dataset is "too easy" and heavily dependent on formatting artifacts rather than deep linguistic differences.

2. **Watermarks:** As seen in Case 2 (API Error), the model likely learned specific "watermarks" (e.g., "OpenAI", system errors) rather than the subtleties of human vs. AI writing styles.

3. **Generalization Concern:** The model is likely overfitted to the specific quirks of this dataset. It may perform poorly on "clean" AI text generated by newer models (e.g., GPT-4o) that do not leave such obvious artifacts.

### Future Improvements

With more time and resources, I would:

- **Test Out-of-Distribution:** Evaluate the model on a new, clean dataset generated by state-of-the-art models to test true generalization.
- **Data Augmentation:** Remove obvious artifacts (like API errors or Markdown headers) from the training data to force the model to learn semantic features (e.g., perplexity, burstiness).
- **Interpretability:** Use attention visualization tools to verify which tokens the model focuses on for its decisions.

---

## 6. Usage

### Installation

Install the required dependencies using pip:

```bash
pip install -r requirements.txt
```

> **Key dependencies:** `torch`, `transformers`, `pandas`, `scikit-learn`, `tqdm`

### Running the Project

1. Clone this repository.
2. Navigate to the `notebooks/` directory.
3. Run `pipeline_demo.ipynb` using Jupyter Notebook.
4. The notebook handles data loading, preprocessing, training, and evaluation end-to-end.

---

## License

This project was created for educational purposes as part of the CAS2105 course.
