{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d75d68",
   "metadata": {},
   "source": [
    "# 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018ad9c",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 개수: 14642\n",
      "전체 데이터 개수: 14642\n",
      "Train 개수 (60%): 8784\n",
      "Valid 개수 (20%): 2929\n",
      "Test  개수 (20%): 2929\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"gsingh1-py/train\")\n",
    "\n",
    "human_df = pd.DataFrame({'text': data.iloc[:, 1], 'label': 0})\n",
    "ai_series = data.iloc[:, 2:].stack().reset_index(drop=True)\n",
    "\n",
    "# AI 데이터 샘플링 (Human 개수만큼)\n",
    "ai_subset = ai_series.sample(n=len(human_df), random_state=42)\n",
    "ai_df = pd.DataFrame({'text': ai_subset, 'label': 1})\n",
    "\n",
    "# 데이터 합치기\n",
    "final_data = pd.concat([human_df, ai_df], axis=0, ignore_index=True)\n",
    "final_data = final_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# [중요] 결측치 제거 및 문자열 변환 (에러 방지 필수 단계)\n",
    "final_data['text'] = final_data['text'].fillna(\"\").astype(str)\n",
    "\n",
    "print(f\"전체 데이터 개수: {len(final_data)}\")\n",
    "\n",
    "# 2. Train/Test Split\n",
    "# 변수명을 X_train_text로 명확히 하여 혼동 방지\n",
    "X_remaining, X_test_text, y_remaining, y_test = train_test_split(\n",
    "    final_data['text'].values, \n",
    "    final_data['label'].values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2차 분할: 나머지 80% 데이터를 [Train 60% : Validation 20%]로 나눔\n",
    "# (남은 데이터의 1/4인 0.25를 떼어내면 전체의 20%가 됨)\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_remaining, \n",
    "    y_remaining, \n",
    "    test_size=0.25, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"전체 데이터 개수: {len(final_data)}\")\n",
    "print(f\"Train 개수 (60%): {len(X_train_text)}\")\n",
    "print(f\"Valid 개수 (20%): {len(X_val_text)}\")\n",
    "print(f\"Test  개수 (20%): {len(X_test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626203d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qwert\\miniforge3\\envs\\AIpipe\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "# [수정 1] 초기화 시 불필요한 옵션(max_length 등) 제거 -> 에러 해결\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True)\n",
    "\n",
    "# [수정 2] maxlen 고정\n",
    "# 실제 데이터 길이가 아니라 모델 한계(512) 내에서 설정해야 함. \n",
    "# 과제용으로는 128~256 추천 (속도/메모리 효율)\n",
    "maxlen = 256\n",
    "\n",
    "# [수정 3] 고속/메모리 효율적 토큰화 함수 (batch_encode_plus 사용)\n",
    "def tokenize(sentences, tokenizer):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(\n",
    "        sentences.tolist(),      # Numpy 배열을 리스트로 변환하여 전달\n",
    "        add_special_tokens=True,\n",
    "        max_length=maxlen,\n",
    "        padding='max_length',    # maxlen에 맞춰 0으로 채움\n",
    "        truncation=True,         # 넘치면 자름\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='np'      # Numpy Array로 바로 반환 (메모리 절약)\n",
    "    )\n",
    "    \n",
    "    # DistilBERT는 token_type_ids(segments)를 쓰지 않으므로 반환하지 않아도 됨\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0600",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7dcc50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Train set...\n",
      "완료! Train shape: (8784, 256), Label shape: (8784,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing Train set...\")\n",
    "X_train_ids, X_train_masks = tokenize(X_train_text, tokenizer)\n",
    "\n",
    "X_val_ids, X_val_masks = tokenize(X_val_text, tokenizer)\n",
    "print(f\"완료! Train shape: {X_train_ids.shape}, Label shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f853bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3050 4GB Laptop GPU\n",
      "데이터 로더 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 1. GPU가 있는지 확인하고 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_train_ids), \n",
    "    torch.tensor(X_train_masks), \n",
    "    torch.tensor(y_train)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(X_val_ids), \n",
    "    torch.tensor(X_val_masks), \n",
    "    torch.tensor(y_val)\n",
    ")\n",
    "\n",
    "# 4. 데이터 로더 생성 (배달원)\n",
    "# 학습 때는 데이터를 섞어야(Shuffle) 편향되지 않습니다.\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, \n",
    "    sampler=RandomSampler(train_data), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# 평가 때는 섞을 필요가 없습니다.\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_data, \n",
    "    sampler=SequentialSampler(validation_data), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "print(\"데이터 로더 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc86f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73477159",
   "metadata": {},
   "source": [
    "## Naive Baseline implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_baseline(sentence):\n",
    "    sentence = sentence.lower() # 소문자로 변환\n",
    "    if any(word in sentence for word in ['**', '##', 'title']):\n",
    "        return 1 # AI 라벨\n",
    "    else:\n",
    "        # if not, randomly choose between Human(0) and AI(1)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3001ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이스라인 평가 결과:\n",
      "정확도: 0.9249\n"
     ]
    }
   ],
   "source": [
    "# 5. 나이브 베이스라인 평가\n",
    "naive_preds = [naive_baseline(sent) for sent in X_test_text]\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"나이브 베이스라인 평가 결과:\")\n",
    "print(f\"정확도: {accuracy_score(y_test, naive_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933459e1",
   "metadata": {},
   "source": [
    "## AI PIPELINE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2, # 0: Human, 1: AI\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f677c22",
   "metadata": {},
   "source": [
    "## Training (Model use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc94d7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [09:24<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.04\n",
      "Running Validation...\n",
      "  Validation Accuracy: 1.00\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [15:38<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.00\n",
      "Running Validation...\n",
      "  Validation Accuracy: 1.00\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [15:42<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.00\n",
      "Running Validation...\n",
      "  Validation Accuracy: 1.00\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    model.train() # 학습 모드 설정\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # 배치 단위로 학습 진행\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # 1. 배치를 GPU로 이동\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # 2. 그래디언트 초기화\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # 3. Forward Pass (모델 예측)\n",
    "        output = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            labels=b_labels\n",
    "        )\n",
    "        \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 4. Backward Pass (역전파)\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. 파라미터 업데이트\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 그래디언트 폭주 방지\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval() # 평가 모드 설정 (Dropout 끔)\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad(): # 평가 때는 그래디언트 계산 안 함 (메모리 절약)\n",
    "            output = model(\n",
    "                b_input_ids, \n",
    "                attention_mask=b_input_mask, \n",
    "                labels=b_labels\n",
    "            )\n",
    "            \n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # GPU에 있는 데이터를 CPU로 옮겨서 정확도 계산\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    print(f\"  Validation Accuracy: {total_eval_accuracy/len(validation_dataloader):.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb85021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Test set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing Test set...\")\n",
    "X_test_ids, X_test_masks = tokenize(X_test_text, tokenizer)\n",
    "\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(X_test_ids), \n",
    "    torch.tensor(X_test_masks), \n",
    "    torch.tensor(y_test)\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, \n",
    "    sampler=SequentialSampler(test_data), \n",
    "    batch_size=16  # 학습 때 쓴 batch_size와 같거나 달라도 상관없습니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bad677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== 최종 테스트 데이터 평가 (Final Test) ========\n",
      "  Final Test Loss: 0.0001\n",
      "  Final Test Accuracy: 1.0000\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======== 최종 테스트 데이터 평가 (Final Test) ========\")\n",
    "\n",
    "model.eval() # 모델을 평가 모드로 전환 (Dropout, BatchNorm 등이 고정됨)\n",
    "\n",
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    # 데이터를 GPU로 이동\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    # 그래디언트 계산 안 함 (메모리 절약, 속도 향상)\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            labels=b_labels\n",
    "        )\n",
    "    \n",
    "    loss = output.loss\n",
    "    logits = output.logits\n",
    "\n",
    "    # CPU로 데이터 이동 (Numpy 변환을 위해)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # 결과 누적\n",
    "    total_test_loss += loss.item()\n",
    "    total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    # (선택사항) 나중에 혼동 행렬 등을 그리기 위해 예측값 저장\n",
    "    all_predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "    all_true_labels.extend(label_ids.flatten())\n",
    "\n",
    "# 3. 최종 결과 계산 및 출력\n",
    "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "print(f\"  Final Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Final Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "882c7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_qualitative_examples(X_text, y_true, naive_preds, model_preds, tokenizer, model, device, count=3):\n",
    "    \"\"\"\n",
    "    베이스라인과 AI 모델의 예측이 서로 다른 흥미로운 사례를 찾아 출력합니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Qualitative Analysis (Method Comparison) {'='*20}\")\n",
    "    \n",
    "    # 1. 두 모델의 예측이 다른 인덱스 찾기 (가장 분석 가치가 높음)\n",
    "    diff_indices = [i for i in range(len(y_true)) if naive_preds[i] != model_preds[i]]\n",
    "    \n",
    "    # 만약 다른 경우가 별로 없다면, AI 모델이 틀린 경우를 추가로 찾음\n",
    "    if len(diff_indices) < count:\n",
    "        wrong_indices = [i for i in range(len(y_true)) if model_preds[i] != y_true[i]]\n",
    "        diff_indices.extend(wrong_indices)\n",
    "        # 중복 제거 및 앞에서부터 필요한 만큼 자르기\n",
    "        diff_indices = list(set(diff_indices))\n",
    "    \n",
    "    # 그래도 부족하면 그냥 앞에서부터 채움\n",
    "    if len(diff_indices) < count:\n",
    "        diff_indices = list(range(count))\n",
    "        \n",
    "    selected_indices = diff_indices[:count]\n",
    "    \n",
    "    label_map = {0: \"Human\", 1: \"AI\"}\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        text_sample = X_text[idx]\n",
    "        true_lbl = y_true[idx]\n",
    "        naive_lbl = naive_preds[idx]\n",
    "        ai_lbl = model_preds[idx]\n",
    "        \n",
    "        # AI 모델의 확신도(Probability) 계산을 위해 다시 Inference 수행\n",
    "        inputs = tokenizer(text_sample, return_tensors=\"pt\", truncation=True, max_length=256, padding=\"max_length\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        print(f\"\\n[Example Case #{i+1}] Index: {idx}\")\n",
    "        print(f\"Text (Snippet): \\\"{text_sample[:150]}...\\\"\") # 텍스트가 너무 길면 자름\n",
    "        print(f\"-\" * 60)\n",
    "        print(f\"Ground Truth   : {label_map[true_lbl]} ({true_lbl})\")\n",
    "        print(f\"Naive Baseline : {label_map[naive_lbl]} ({naive_lbl}) \\t <- {'Correct' if naive_lbl == true_lbl else 'Wrong'}\")\n",
    "        print(f\"AI Pipeline    : {label_map[ai_lbl]} ({ai_lbl}) \\t <- {'Correct' if ai_lbl == true_lbl else 'Wrong'}\")\n",
    "        print(f\"AI Confidence  : Human {probs[0]:.4f} vs AI {probs[1]:.4f}\")\n",
    "        \n",
    "        # 짧은 코멘트 생성 (보고서 작성용 힌트)\n",
    "        print(f\"Discussion Hint: \", end=\"\")\n",
    "        if naive_lbl != ai_lbl and ai_lbl == true_lbl:\n",
    "            print(\"The AI model successfully captured nuances that the rule-based baseline missed.\")\n",
    "        elif naive_lbl == true_lbl and ai_lbl != true_lbl:\n",
    "            print(\"The AI model failed on this example, while the simple baseline got it right.\")\n",
    "        else:\n",
    "            print(\"Both models showed similar performance on this difficult/easy example.\")\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f096759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Qualitative Analysis (Method Comparison) ====================\n",
      "\n",
      "[Example Case #1] Index: 35\n",
      "Text (Snippet): \" 400000 A Tale of Three Housing Markets\n",
      "\n",
      "New York Mississippi Maryland three states one price point vastly different realities \n",
      "\n",
      "The median home price...\"\n",
      "------------------------------------------------------------\n",
      "Ground Truth   : AI (1)\n",
      "Naive Baseline : Human (0) \t <- Wrong\n",
      "AI Pipeline    : AI (1) \t <- Correct\n",
      "AI Confidence  : Human 0.0001 vs AI 0.9999\n",
      "Discussion Hint: The AI model successfully captured nuances that the rule-based baseline missed.\n",
      "======================================================================\n",
      "\n",
      "[Example Case #2] Index: 47\n",
      "Text (Snippet): \"Error: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Cau...\"\n",
      "------------------------------------------------------------\n",
      "Ground Truth   : AI (1)\n",
      "Naive Baseline : Human (0) \t <- Wrong\n",
      "AI Pipeline    : AI (1) \t <- Correct\n",
      "AI Confidence  : Human 0.0002 vs AI 0.9998\n",
      "Discussion Hint: The AI model successfully captured nuances that the rule-based baseline missed.\n",
      "======================================================================\n",
      "\n",
      "[Example Case #3] Index: 75\n",
      "Text (Snippet): \"new video loaded:Errol Morris: ‘Demon in the Freezer’\n",
      "transcript\n",
      "Errol Morris: ‘Demon in the Freezer’\n",
      "Smallpox has inflicted untold suffering and deat...\"\n",
      "------------------------------------------------------------\n",
      "Ground Truth   : Human (0)\n",
      "Naive Baseline : AI (1) \t <- Wrong\n",
      "AI Pipeline    : Human (0) \t <- Correct\n",
      "AI Confidence  : Human 0.9999 vs AI 0.0001\n",
      "Discussion Hint: The AI model successfully captured nuances that the rule-based baseline missed.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "naive_preds_test = [naive_baseline(sent) for sent in X_test_text]\n",
    "\n",
    "# 2. 함수 실행\n",
    "# all_predictions는 이전 셀(Final Test)에서 생성된 AI 모델의 예측값 리스트입니다.\n",
    "# all_true_labels는 y_test와 동일합니다.\n",
    "show_qualitative_examples(\n",
    "    X_text=X_test_text, \n",
    "    y_true=y_test, \n",
    "    naive_preds=naive_preds_test, \n",
    "    model_preds=all_predictions,  # 이전 셀의 결과 변수\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    count=3\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
